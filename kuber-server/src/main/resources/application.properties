# ==============================================================================
# Kuber Distributed Cache - Application Configuration
# Copyright (c) 2025-2030, All Rights Reserved
# Ashutosh Sinha | Email: ajsinha@gmail.com
# ==============================================================================

# ==============================================================================
# Base Data Directory Configuration
# ==============================================================================
# All data directories are relative to this base path.
# Override with -Dkuber.base.datadir=/your/path or environment variable KUBER_BASE_DATADIR
# Application Version
kuber.version=2.6.3

# Secure folder for sensitive configuration files (users.json, apikeys.json)
# This folder will be created automatically if it doesn't exist
# If users.json is missing, the application will fail to start
kuber.secure.folder=config/secure

kuber.base.datadir=./kuberdata

# Application Branding
server.app.name=Kuber
server.app.author=Ashutosh Sinha
server.app.email=ajsinha@gmail.com
server.app.copyright=2025-2030
server.app.github=https://github.com/ajsinha/kuber

# Spring Application
spring.application.name=kuber-cache

# Thymeleaf Configuration
spring.thymeleaf.cache=false
spring.thymeleaf.mode=HTML
spring.thymeleaf.encoding=UTF-8
spring.thymeleaf.prefix=classpath:/templates/
spring.thymeleaf.suffix=.html
spring.thymeleaf.check-template-location=true

# Server Configuration
server.port=8080
server.address=0.0.0.0
server.servlet.session.timeout=30m
server.shutdown=graceful
spring.lifecycle.timeout-per-shutdown-phase=30s

# Path Matching - Use AntPathMatcher so %2F (encoded slash) in cache keys
# stays as a single path segment during URL matching. Without this, Spring Boot 3.x
# defaults to PathPatternParser which decodes %2F before matching, breaking
# keys like employee/EMP001. See also: WebMvcConfig, TomcatConfig, SecurityConfig.
spring.mvc.pathmatch.matching-strategy=ant-path-matcher

# Actuator Endpoints
management.endpoints.web.exposure.include=health,info,metrics,prometheus
management.endpoint.health.show-details=always
management.metrics.tags.application=kuber

# ==============================================================================
# Prometheus Metrics Configuration (v1.7.9)
# ==============================================================================
# Enable Prometheus metrics endpoint at /actuator/prometheus
kuber.prometheus.enabled=true
# Metrics refresh interval in milliseconds
kuber.prometheus.update-interval-ms=5000
# Include JVM metrics (memory, GC, threads)
kuber.prometheus.include-jvm-metrics=true
# Include per-region metrics
kuber.prometheus.include-region-metrics=true
# Custom metric prefix
kuber.prometheus.metric-prefix=kuber
# Include latency histograms (may increase memory usage)
kuber.prometheus.include-latency-histograms=false

# ==============================================================================
# Secondary Index Configuration (v1.9.0)
# ==============================================================================
# Path to the index configuration file (YAML format)
# Default: ${kuber.base.datadir}/index.yaml (e.g., ./data/index.yaml)
# This file defines secondary indexes for JSON document search optimization
# The file is auto-created with defaults if it doesn't exist
# kuber.indexing.config-file=${kuber.base.datadir}/index.yaml

# Watch for changes in index configuration file (auto-reload on manual edits)
kuber.indexing.watch-for-changes=true

# Auto-save index configuration when indexes are created/deleted via UI or API
kuber.indexing.auto-save=true

# ==============================================================================
# Index File Watcher Configuration (v1.9.0)
# ==============================================================================
# Enable file-based index operation triggers for automation and scripting.
# When enabled, creating trigger files like kuber.index.<region>.rebuild will
# execute the corresponding index operation.
kuber.indexing.file-watcher-enabled=true

# How often to check for index trigger files (in milliseconds)
kuber.indexing.file-watcher-interval-ms=5000

# Directory to watch for index trigger files (default: current working directory)
# kuber.indexing.file-watcher-directory=.

# Supported trigger file patterns:
#   kuber.index.<region>.rebuild           - Rebuild all indexes for region
#   kuber.index.<region>.drop              - Drop all indexes for region
#   kuber.index.<region>.<field>.rebuild   - Rebuild specific index
#   kuber.index.<region>.<field>.drop      - Drop specific index
#   kuber.index.all.rebuild                - Rebuild ALL indexes
#   kuber.index.<region>.<field>.create.<type> - Create new index (hash/btree/trigram/prefix)

# ==============================================================================
# Index Storage Configuration (v1.9.0)
# ==============================================================================
# Storage mode: HEAP (on-heap, fast, GC pressure), OFFHEAP (slower, zero GC),
# or DISK (slowest, minimal RAM, persistent)
# OFFHEAP recommended for large indexes (>10M entries)
# DISK recommended for very large indexes or memory-constrained environments

# Default storage mode for all index types (HEAP, OFFHEAP, or DISK)
kuber.indexing.default-storage=HEAP

# Per-type storage overrides (HEAP, OFFHEAP, DISK, or DEFAULT to use default-storage)
kuber.indexing.hash-storage=DEFAULT
kuber.indexing.btree-storage=DEFAULT
kuber.indexing.trigram-storage=OFFHEAP
kuber.indexing.prefix-storage=DEFAULT

# Off-heap buffer sizes (only applies when using OFFHEAP storage)
# Initial buffer size per index (default: 16MB)
kuber.indexing.offheap-initial-size=16777216
# Maximum buffer size per index (default: 1GB)
kuber.indexing.offheap-max-size=1073741824

# ==============================================================================
# Disk-Based Index Storage Configuration (v1.9.0)
# ==============================================================================
# IMPORTANT: This section configures where SECONDARY INDEXES are stored,
# NOT where the actual cache DATA is stored.
#
# Data Persistence (kuber.persistence.type): postgresql, mongodb, sqlite, rocksdb, lmdb, memory
# Index Storage (kuber.indexing.disk-backend): rocksdb, lmdb, sqlite (embedded DBs only)
#
# These are INDEPENDENT configurations. Example combinations:
#   - PostgreSQL for data + RocksDB for indexes (recommended for production)
#   - MongoDB for data + LMDB for indexes
#   - RocksDB for data + SQLite for indexes (debugging)
#
# Why no PostgreSQL/MongoDB for index storage?
#   - Indexes need fast local disk access (microseconds, not milliseconds)
#   - Network latency to external DBs would slow down every index lookup
#   - The Hybrid Query Strategy already uses PostgreSQL/MongoDB's native JSON
#     query capabilities when Kuber indexes don't exist (best of both worlds)

# Backend engine for disk-based INDEX storage: rocksdb, lmdb, or sqlite
# rocksdb: Best write performance, LSM-tree architecture, good for write-heavy workloads
# lmdb:    Best read performance (memory-mapped files), good for read-heavy workloads
# sqlite:  Most portable (pure Java), SQL-queryable for debugging
kuber.indexing.disk-backend=rocksdb

# Directory for disk-based index storage
kuber.indexing.disk-directory=${kuber.base.datadir}/indexes

# Enable write-ahead logging for disk indexes (improves durability, reduces performance)
# Default: false (indexes can be rebuilt from data if corrupted)
kuber.indexing.disk-wal-enabled=false

# Sync writes to disk immediately (maximum durability, slower writes)
# Default: false (buffered writes, faster but may lose recent index updates on crash)
kuber.indexing.disk-sync-writes=false

# Cache size for disk-based indexes in MB (higher = better read performance)
kuber.indexing.disk-cache-size-mb=64

# Enable bloom filters for disk indexes (improves lookup performance for non-existent keys)
kuber.indexing.disk-bloom-filter-enabled=true

# Reuse existing disk index data on startup (skip rebuild if indexes exist)
# When true: Fast startup, reuse existing index data
# When false: Always rebuild indexes from persistence on startup
kuber.indexing.disk-reuse-on-startup=true

# ==============================================================================
# Hybrid Query Strategy Configuration (v1.9.0)
# ==============================================================================
# When no Kuber secondary index exists for a query, the hybrid strategy
# attempts to use native database queries (PostgreSQL GIN, MongoDB, SQLite JSON1)
# instead of a full in-memory scan.
#
# Query Priority:
# 1. Kuber Secondary Index (if defined) - fastest, O(1) or O(log n)
# 2. Native Database Query (if supported) - fast, uses DB indexes
# 3. Full Scan (fallback) - slowest, scans all entries

# Enable hybrid query strategy (use native DB queries when no Kuber index exists)
kuber.indexing.hybrid-query-enabled=true

# Threshold for using native database queries (entry count)
# If the region has fewer entries than this threshold, use full scan instead
# (full scan is often faster for small datasets due to less overhead)
kuber.indexing.hybrid-query-threshold=10000

# ==============================================================================
# Parallel JSON Search Configuration (v1.9.0)
# ==============================================================================
# Enable parallel JSON search for improved performance on large datasets
# When enabled, searches on datasets larger than the threshold use multiple threads
kuber.search.parallel-enabled=true

# Number of threads for parallel search (default: 8)
# Set to 0 to use number of available CPU cores
kuber.search.thread-count=8

# Minimum number of keys to trigger parallel search
# Datasets smaller than this use sequential search (parallel overhead not worth it)
kuber.search.parallel-threshold=1000

# Search timeout in seconds
# If a search takes longer than this, partial results are returned
kuber.search.timeout-seconds=60

# Cache compiled regex patterns for performance
kuber.search.cache-regex-patterns=true

# Maximum number of cached regex patterns
kuber.search.max-cached-patterns=1000

# Logging Configuration
logging.level.root=INFO
logging.level.com.kuber=DEBUG
logging.level.org.apache.mina=WARN
logging.level.org.springframework.security=WARN
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n

# ==============================================================================
# Kuber Cache Configuration
# ==============================================================================

# Network Configuration
kuber.network.port=6380
kuber.network.bind-address=0.0.0.0
kuber.network.decoder-max-line-length=1048576

# Session idle timeout in milliseconds. Sessions are closed after this period of inactivity.
# Default: 300000 (5 minutes). Use CLIENT SETTIMEOUT <seconds> at runtime to adjust per-session.
# Set to 0 to disable timeout (not recommended for production).
kuber.network.connection-timeout-ms=300000

kuber.network.read-timeout-ms=30000
kuber.network.read-buffer-size=2048
kuber.network.write-buffer-size=2048
kuber.network.max-connections=10000
kuber.network.io-processor-count=0

# Cache Configuration
kuber.cache.max-memory-entries=100000

# Maximum key length in bytes. Keys exceeding this limit will be rejected.
# The key and value will be logged for debugging. Applies to ALL persistence stores.
# Default: 1024 bytes. Range: 1-65536 bytes. Set higher for longer keys (e.g., URLs, paths).
kuber.cache.max-key-length-bytes=1024

# persistent-mode: true = sync writes to MongoDB, false = async writes
# Note: Regions are ALWAYS persisted to MongoDB regardless of this setting
kuber.cache.persistent-mode=false
kuber.cache.persistence-batch-size=100
kuber.cache.persistence-interval-ms=1000
kuber.cache.default-ttl-seconds=-1
kuber.cache.eviction-policy=LRU
kuber.cache.ttl-cleanup-interval-seconds=60
kuber.cache.enable-statistics=true

# Off-Heap Key Index Configuration (v1.2.2)
# Store keys in direct memory (DRAM) outside Java heap for zero GC pressure
# Recommended for millions of keys to eliminate GC pauses affecting key lookups
kuber.cache.off-heap-key-index=false
# Initial buffer size per region in MB (grows automatically up to max)
kuber.cache.off-heap-key-index-initial-size-mb=16
# Maximum buffer size per region in MB (supports >2GB using segmented buffers - v1.3.2)
kuber.cache.off-heap-key-index-max-size-mb=8192

# ==============================================================================
# Warm Object Configuration (v1.7.9)
# ==============================================================================
# Maintain a minimum number of "warm" (in-memory) objects per region.
# This ensures frequently accessed data remains in memory for fast access.
#
# Enable warm object maintenance
kuber.cache.warm-objects-enabled=true
# Check interval for warm object maintenance (in milliseconds)
kuber.cache.warm-object-check-interval-ms=60000
# Batch size for loading warm objects from disk
kuber.cache.warm-object-load-batch-size=1000

# Per-region warm object counts (example - uncomment and customize as needed)
# kuber.cache.region-warm-object-counts.trade=100000
# kuber.cache.region-warm-object-counts.reference=50000
# kuber.cache.region-warm-object-counts.session=10000
# kuber.cache.region-warm-object-counts.default=5000

# MongoDB Configuration
kuber.mongo.uri=mongodb://localhost:27017
kuber.mongo.database=kuber
kuber.mongo.connection-pool-size=50
kuber.mongo.connect-timeout-ms=10000
kuber.mongo.socket-timeout-ms=30000
kuber.mongo.server-selection-timeout-ms=30000
kuber.mongo.write-concern=ACKNOWLEDGED

# ==============================================================================
# Persistence Configuration
# ==============================================================================
# Supported types: rocksdb (default), lmdb, mongodb, sqlite, postgresql, aerospike, memory
# RocksDB is recommended for production - excellent performance with built-in compaction
# Changed from LMDB to RocksDB as default in v1.7.9
kuber.persistence.type=rocksdb

# Individual write mode for PUT/SET operations (v1.3.10)
# false (default) = ASYNC: Memory updated first, disk write in background
#                   Faster (10-100x), eventually consistent, slight data loss risk on crash
# true = SYNC: Wait for disk write before returning
#        Slower (~1-5ms per write), maximum durability
# Note: Autoload batch operations ALWAYS use async mode (batch writes with WAL, not synced)
#       regardless of this setting for optimal bulk load performance
kuber.persistence.sync-individual-writes=false

# SQLite Configuration (used when persistence.type=sqlite)
kuber.persistence.sqlite.path=${kuber.base.datadir}/data/kuber.db

# PostgreSQL Configuration (used when persistence.type=postgresql)
kuber.persistence.postgresql.url=jdbc:postgresql://localhost:5432/kuber
kuber.persistence.postgresql.username=postgres
kuber.persistence.postgresql.password=postgres
kuber.persistence.postgresql.pool-size=10
kuber.persistence.postgresql.min-idle=2
kuber.persistence.postgresql.connection-timeout-ms=30000
kuber.persistence.postgresql.idle-timeout-ms=600000
kuber.persistence.postgresql.max-lifetime-ms=1800000

# RocksDB Configuration (used when persistence.type=rocksdb) - DEFAULT (v1.7.9+)
# RocksDB is the default persistence engine as of v1.7.9
# Recommended for production due to:
# - Excellent read/write performance with LSM tree architecture
# - Built-in compaction to reclaim disk space automatically
# - Robust crash recovery and data integrity
# - Well-suited for high write workloads
kuber.persistence.rocksdb.path=${kuber.base.datadir}/data/rocksdb
# Enable automatic compaction to reclaim disk space
kuber.persistence.rocksdb.compaction-enabled=true
# Cron expression for scheduled compaction (default: 2:00 AM daily)
# Format: second minute hour day-of-month month day-of-week
# Examples: "0 0 2 * * ?" (2AM daily), "0 0 3 * * SUN" (3AM Sundays), "0 0 */6 * * ?" (every 6 hours)
kuber.persistence.rocksdb.compaction-cron=0 0 2 * * ?

# ZooKeeper Configuration (for replication)
kuber.zookeeper.enabled=false
kuber.zookeeper.connect-string=localhost:2181
kuber.zookeeper.session-timeout-ms=30000
kuber.zookeeper.connection-timeout-ms=10000
kuber.zookeeper.base-path=/kuber
kuber.zookeeper.retry-base-sleep-ms=1000
kuber.zookeeper.retry-max-attempts=3

# Replication Configuration
kuber.replication.oplog-capacity=100000
kuber.replication.sync-batch-size=5000
kuber.replication.sync-interval-ms=500
kuber.replication.sync-timeout-ms=30000
kuber.replication.heartbeat-interval-ms=5000
kuber.replication.primary-check-interval-ms=10000
kuber.replication.connect-timeout-ms=5000
kuber.replication.read-timeout-ms=30000
kuber.replication.advertised-address=localhost
# kuber.replication.auth-token=my-secret-replication-token

# Security Configuration
# Users file is required - application will fail to start if missing
kuber.security.users-file=${kuber.secure.folder}/users.json
# Roles file for RBAC - auto-created with defaults if missing (v1.7.3)
kuber.security.roles-file=${kuber.secure.folder}/roles.json
# API Keys file location (optional - will be created if missing)
kuber.security.api-keys-file=${kuber.secure.folder}/apikeys.json
kuber.security.session-timeout-minutes=30
kuber.security.redis-password=
# Enable RBAC authorization checks on cache operations (v1.7.3)
kuber.security.rbac-enabled=true
# Auto-create region roles (readonly, readwrite, full) when new region is created (v1.7.3)
kuber.security.auto-create-region-roles=true

# ==============================================================================
# Autoload Configuration
# ==============================================================================
# Automatically load data from CSV and JSON files placed in inbox folder
# Files are processed sequentially for data consistency (v1.3.8)
# Batch writes used for better performance (v1.3.9)
kuber.autoload.enabled=true
kuber.autoload.directory=${kuber.base.datadir}/autoload
kuber.autoload.scan-interval-seconds=60
kuber.autoload.max-records-per-file=0
kuber.autoload.create-directories=true
kuber.autoload.file-encoding=UTF-8
# Batch size for bulk writes to persistence store (default: 32768)
# Records are accumulated and written in batches for better performance
# Batch writes use async mode (WAL written but not synced per batch)
kuber.autoload.batch-size=32768
# Percentage of cache capacity to warm after autoload completes (default: 10)
# This proactively loads data into memory in a background thread
# Set to 0 to disable automatic warming (rely on lazy loading via GET)
kuber.autoload.warm-percentage=0
# Normalize all text values to US-ASCII during autoload (v1.7.9)
# Converts: accented chars (é→e), special chars (ß→ss), currency (€→EUR), etc.
# Set to false to preserve original Unicode characters
kuber.autoload.ascii-normalize=true
# Also normalize cache keys to ASCII (only applies when ascii-normalize=true)
kuber.autoload.ascii-normalize-keys=true

# ==============================================================================
# Backup and Restore Configuration (v1.4.1)
# ==============================================================================
# Automatic backup and restore for all persistence stores (except MEMORY).
# v2.2.0: Now supports RocksDB, LMDB, PostgreSQL, SQLite, MongoDB, Aerospike.

# Enable periodic backup of all regions
kuber.backup.enabled=true

# Directory where backup files are stored
# Each backup file: <region>.<timestamp>.backup.gz
kuber.backup.backup-directory=${kuber.base.datadir}/backup

# Directory to monitor for restore files
# Place backup files here to trigger automatic restore
kuber.backup.restore-directory=${kuber.base.datadir}/restore

# Cron expression for scheduled backups (default: 11:00 PM daily)
# Format: second minute hour day-of-month month day-of-week
# Examples:
#   0 0 23 * * *    - 11:00 PM daily (default)
#   0 0 2 * * *     - 2:00 AM daily
#   0 0 */6 * * *   - Every 6 hours
#   0 30 1 * * SUN  - 1:30 AM every Sunday
kuber.backup.cron=0 0 23 * * *

# Maximum number of backup files to keep per region (0 = keep all)
kuber.backup.max-backups-per-region=10

# Whether to create directories if they don't exist
kuber.backup.create-directories=true

# Batch size for reading/writing entries during backup/restore
kuber.backup.batch-size=10000

# File encoding for backup files
kuber.backup.file-encoding=UTF-8

# Whether to compress backup files (gzip)
kuber.backup.compress=true

# ==============================================================================
# Graceful Shutdown Configuration (v1.3.4)
# ==============================================================================
# Kuber supports multiple shutdown methods for graceful termination:
# 1. File-based: Create a shutdown signal file (touch ./kuberdata/kuber.shutdown)
# 2. REST API: POST /api/admin/shutdown with API key
# 3. SIGTERM: Standard Unix signal (handled by Spring)

# Enable file-based shutdown monitoring
# When enabled, Kuber watches for a shutdown signal file
kuber.shutdown.file-enabled=true

# Path to the shutdown signal file (relative or absolute)
# Create this file to trigger graceful shutdown: touch ${kuber.base.datadir}/kuber.shutdown
# The file is automatically deleted after shutdown is initiated
kuber.shutdown.file-path=${kuber.base.datadir}/kuber.shutdown

# How often to check for the shutdown file (milliseconds)
# Minimum: 1000ms, Default: 5000ms (5 seconds)
kuber.shutdown.check-interval-ms=5000

# Enable REST API shutdown endpoint
# When enabled, POST /api/admin/shutdown triggers graceful shutdown
# Requires valid API key in X-API-Key header
kuber.shutdown.api-enabled=true

# Delay between shutdown phases (seconds)
# Each phase waits this long before proceeding to ensure clean resource release
# Phases: Signal → Stop Autoload → Stop Redis → Stop Publishing → Sync → Persist → Close
# Total shutdown time ≈ (number of phases) × phase-delay-seconds
# Default: 5 seconds (total ~35-40 seconds for complete shutdown)
kuber.shutdown.phase-delay-seconds=5

# ==============================================================================
# LMDB Configuration (used when persistence.type=lmdb)
# ==============================================================================
# LMDB (Lightning Memory-Mapped Database) - extremely fast reads via mmap
# Alternative to RocksDB when:
# - Zero-copy reads via memory mapping are needed (fastest read performance)
# - ACID transactions with MVCC (crash-safe)
# - No recovery needed after crash (copy-on-write B+ tree)
# - Simple deployment (no external services required)
# Note: RocksDB is now the default as of v1.7.9 due to better compaction
kuber.persistence.lmdb.path=${kuber.base.datadir}/data/lmdb

# Maximum database size - VIRTUAL ADDRESS SPACE, NOT disk or RAM allocation!
#
# IMPORTANT: This is safe to set very large (1TB+) on 64-bit systems because:
# - Only reserves virtual address space (64-bit has 128TB+ available)
# - Actual disk file grows dynamically as data is written
# - Physical RAM usage managed by OS page cache (only active pages)
# - Example: 1TB map-size with 5GB of data uses ~5GB on disk, not 1TB
#
# If you see "Environment mapsize reached" error, increase this and restart.
#
# Common values:
#   100GB = 107374182400
#   500GB = 536870912000
#   1TB   = 1099511627776 (default)
#   2TB   = 2199023255552
#   4TB   = 4398046511104
kuber.persistence.lmdb.map-size=1099511627776

# ==============================================================================
# Aerospike Configuration (used when persistence.type=aerospike)
# ==============================================================================
# Aerospike - High-performance distributed NoSQL database (v1.9.0)
# Best for:
# - Sub-millisecond latency requirements
# - Horizontal scaling across multiple nodes
# - Native TTL support (automatic record expiration)
# - Flash/SSD optimized storage
#
# Prerequisites:
# 1. Install Aerospike server (Community or Enterprise Edition)
# 2. Create namespace in aerospike.conf:
#    namespace kuber {
#        replication-factor 2
#        memory-size 4G
#        default-ttl 0  # or specific TTL
#        storage-engine device {
#            file /opt/aerospike/data/kuber.dat
#            filesize 16G
#        }
#    }
# 3. Start Aerospike: sudo systemctl start aerospike

# Cluster hosts (comma-separated for multiple nodes)
kuber.persistence.aerospike.hosts=localhost:3000

# Namespace (must be pre-configured on Aerospike server)
kuber.persistence.aerospike.namespace=kuber

# Authentication (leave empty if security not enabled)
kuber.persistence.aerospike.username=
kuber.persistence.aerospike.password=

# Connection settings
kuber.persistence.aerospike.connection-timeout-ms=5000
kuber.persistence.aerospike.socket-timeout-ms=30000
kuber.persistence.aerospike.max-conns-per-node=300
kuber.persistence.aerospike.conn-pools-per-node=1

# Write policy
# send-key: Store user key with record (required for scan to return keys)
kuber.persistence.aerospike.send-key=true

# commit-level: COMMIT_ALL (all replicas) or COMMIT_MASTER (master only)
kuber.persistence.aerospike.commit-level=COMMIT_ALL

# Read policy
# replica: SEQUENCE, MASTER, MASTER_PROLES, RANDOM
kuber.persistence.aerospike.replica=SEQUENCE

# Default TTL: -1 (namespace default), -2 (never expire), >0 (seconds)
kuber.persistence.aerospike.default-ttl=-1

# Memory Management Configuration
# Enable automated memory watcher for 24x7 operation
kuber.cache.memory-watcher-enabled=true
# High watermark - start evicting when heap exceeds this percentage
kuber.cache.memory-high-watermark-percent=85
# Low watermark - stop evicting when heap drops below this percentage
kuber.cache.memory-low-watermark-percent=50
# Number of entries to evict per batch
kuber.cache.memory-eviction-batch-size=1000
# Interval between memory checks (milliseconds)
kuber.cache.memory-watcher-interval-ms=5000

# ==============================================================================
# Count-Based Value Cache Limiting (v1.7.4)
# ==============================================================================
# Limits the number of values kept in memory per region, independent of
# memory pressure. Uses the LOWER of percentage-based or absolute limit.
#
# Example: Region with 100,000 keys
#   - 20% limit = 20,000 max values in memory
#   - Absolute limit = 10,000
#   - Effective limit = 10,000 (lower wins)
#
# Example: Region with 1,000 keys
#   - 20% limit = 200 max values in memory
#   - Absolute limit = 10,000
#   - Effective limit = 200 (lower wins)

# Enable count-based value cache limiting (default: true)
kuber.cache.value-cache-limit-enabled=true

# Maximum percentage of total keys to keep values in memory (default: 20)
kuber.cache.value-cache-max-percent=20

# Maximum absolute number of values per region in memory (default: 10000)
kuber.cache.value-cache-max-entries=10000

# Interval between count-based limit checks in milliseconds (default: 30000)
kuber.cache.value-cache-limit-check-interval-ms=30000

# ==============================================================================
# EVENT PUBLISHING CONFIGURATION (v1.2.7, updated v2.2.0)
# ==============================================================================
#
# Kuber publishes cache events to message brokers and file destinations.
# All configuration is managed via external JSON files for easy maintenance:
#
# 1. BROKER DEFINITIONS (config/message_brokers.json)
#    - Kafka, ActiveMQ, RabbitMQ, IBM MQ, and File broker definitions
#    - SSL/TLS configuration per broker
#    - Override path with kuber.publishing.broker-config-file
#
# 2. REGION PUBLISHING CONFIG (config/event_publishing.json)
#    - Maps regions to broker destinations with topics/queues
#    - Override path with kuber.publishing.region-config-file
#
# Brokers and regions from JSON files override same-named entries
# from application.properties (backward compatible).
#
# ==============================================================================

# Global publishing settings
kuber.publishing.thread-pool-size=4
kuber.publishing.queue-capacity=10000

# External JSON config files
kuber.publishing.broker-config-file=config/message_brokers.json
kuber.publishing.region-config-file=config/event_publishing.json

# ==============================================================================
# REQUEST/RESPONSE MESSAGING (v1.7.0, updated v2.2.0)
# ==============================================================================
# Request/Response Messaging is configured via an external JSON file.
# The file is hot-reloaded on change — updates take effect without restart.
#
# Default location: config/request_response.json (relative to working directory)
# Override with: kuber.messaging.request-response-config-file=/path/to/custom.json
#
# Admin UI: /admin/messaging
# REST API: POST /api/v1/messaging/enable or /api/v1/messaging/disable
# ==============================================================================
kuber.messaging.request-response-config-file=config/request_response.json

# ==================== Index Rebuild Optimization (v2.2.0) ====================
# Batch size for loading documents during index rebuild
# Larger batches are more efficient but use more memory
kuber.indexing.rebuild-batch-size=1000

# Number of parallel batches during rebuild
kuber.indexing.rebuild-parallel-batches=4

# Maximum memory budget for indexes in MB (0 = unlimited)
kuber.indexing.max-memory-mb=0

# String interning threshold for common values
kuber.indexing.intern-threshold=100

# Stream from persistence during index rebuild (avoids cache eviction pressure)
# Recommended: true for regions with >100K entries
kuber.indexing.rebuild-from-persistence=true
