\documentclass[11pt,twocolumn]{article}

% ============================================================
% Packages
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage[margin=0.75in]{geometry}
\usepackage{caption}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{fancyhdr}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% ============================================================
% Header/Footer (matching DishtaYantra template)
% ============================================================
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Sinha, 2025 Kuber Research Paper}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% ============================================================
% Title
% ============================================================
\title{%
  \textbf{Kuber: A Lightweight Multi-Protocol Distributed Cache\\
  for Low-Volume Workloads with Hybrid Memory Architecture\\
  and Integrated Message Broker Access}
}

\author{
  Ashutosh Sinha \\
  Independent Researcher \\
  \texttt{ajsinha@gmail.com} \\[0.5em]
  Repository: \url{https://github.com/ajsinha/kuber}
}

\date{}

% ============================================================
\begin{document}
\maketitle
\thispagestyle{fancy}

% ============================================================
% Abstract
% ============================================================
\begin{abstract}
Enterprise caching infrastructure has long been dominated by systems designed for internet-scale
workloads---Redis, Memcached, Hazelcast, and Apache Ignite---requiring substantial operational
investment in cluster management, distributed consensus, and specialized expertise. However,
a significant segment of the market---development teams, departmental applications, early-stage
startups, and compliance-sensitive environments---requires capable caching infrastructure at
far lower volumes, where the complexity and cost of internet-scale systems represent unnecessary
overhead. We present Kuber, a lightweight open-source distributed cache that addresses this
underserved segment through three architectural innovations: (1) a hybrid memory model inspired
by Aerospike that maintains all keys in memory with tiered value storage across Caffeine LRU cache
and pluggable persistence backends (RocksDB, LMDB, SQLite, MongoDB, PostgreSQL), (2) a
multi-protocol access layer supporting Redis RESP protocol, REST HTTP/JSON API, and message
broker channels (Kafka, RabbitMQ, ActiveMQ, IBM MQ) through a single-JAR deployment, and
(3) an integrated web administration console with role-based access control, API key management,
and structured event publishing. Kuber provides Redis protocol compatibility for drop-in migration,
JSON-native document querying with secondary indexing, SSL/TLS security, and replication---all
without external dependencies beyond a JVM runtime. This paper presents Kuber's architecture,
capabilities, and a qualitative comparison with existing systems, positioning it as a practical
low-cost alternative for deployments not requiring internet-scale infrastructure.

\medskip
\noindent\textbf{Keywords:} Distributed caching, Key-value store, Hybrid memory architecture,
Multi-protocol access, Redis compatibility, JSON document store, Message broker integration,
Low-volume workloads, Open-source infrastructure
\end{abstract}

% ============================================================
% 1. Introduction
% ============================================================
\section{Introduction}

The proliferation of microservice architectures, real-time analytics, and data-intensive applications
has made caching infrastructure a critical component of modern software systems. Organizations
routinely deploy caching layers to reduce database load, accelerate response times, and enable
ephemeral data storage for session management, rate limiting, and inter-service communication.

The caching landscape is dominated by systems engineered for internet-scale deployments.
Redis~\cite{carlson2013} serves millions of operations per second across distributed clusters.
Memcached~\cite{fitzpatrick2004} powers Facebook's multi-tiered caching infrastructure handling
billions of requests daily~\cite{nishtala2013}. Hazelcast~\cite{johns2015} and Apache
Ignite~\cite{ignite2024} provide distributed in-memory data grids with sophisticated cluster
management. These systems have proven their value at massive scale, but their architectural
assumptions impose significant overhead on smaller deployments.

Existing systems present three fundamental challenges for low-volume workloads:

1. \textbf{Operational complexity} in deployment and management requires dedicated infrastructure
teams. Redis Cluster demands a minimum of six nodes for high availability. Hazelcast and Ignite
require careful tuning of distributed consensus and partition management. Even single-node Redis
deployments require external tools for administration, monitoring, and security configuration.

2. \textbf{Licensing uncertainty} has disrupted the ecosystem. Redis Ltd. transitioned from BSD
to RSALv2/SSPLv1 in March 2024~\cite{redis2024license}, then added AGPLv3 as a third option
with Redis 8 in May 2025~\cite{redis2025license}. This created community forks including
Valkey~\cite{valkey2024} (Linux Foundation) and Redict (LGPL), fragmenting the ecosystem
and introducing adoption risk for organizations with strict licensing requirements.

3. \textbf{Feature stratification} reserves essential capabilities behind commercial licenses.
Hazelcast's enterprise features---including TLS/SSL, role-based access control, and advanced
persistence---require commercial licensing. Apache Ignite's full capabilities similarly require
GridGain's commercial distribution. Organizations needing security and persistence for modest
workloads face disproportionate licensing costs.

We present Kuber\footnote{Sanskrit for ``Earth'' / ``Treasure'' --- reflecting the cache's
role as a repository for valued data.}, a lightweight distributed cache that addresses these
limitations through a single-JAR deployment providing multi-protocol access, hybrid memory
architecture, integrated web administration, and comprehensive security---all under an
open-source license suitable for low-to-moderate volume workloads.

\subsection{Contributions}

This paper makes the following contributions:

\begin{itemize}[nosep]
  \item \textbf{Kuber Framework:} An open-source multi-protocol distributed cache
    combining Redis RESP compatibility, REST API, and message broker access in a
    single-JAR deployment (\S\ref{sec:arch})
  \item \textbf{Hybrid Memory Architecture:} An Aerospike-inspired design with in-memory
    keys and tiered value storage across five pluggable persistence backends (\S\ref{sec:hybrid})
  \item \textbf{JSON Document Store:} Native JSON operations with JSONPath querying,
    criteria-based search, and secondary indexing (\S\ref{sec:capabilities})
  \item \textbf{Comprehensive Literature Survey:} Analysis of 10+ caching systems spanning
    key-value stores, in-memory data grids, and emerging alternatives (\S\ref{sec:related})
  \item \textbf{Qualitative Feature Comparison:} Systematic comparison across 19 capability
    dimensions (\S\ref{sec:comparison})
\end{itemize}

% ============================================================
% 2. Background and Motivation
% ============================================================
\section{Background and Motivation}

\subsection{The Caching Landscape}

Modern caching systems span a spectrum from single-node in-process caches to globally
distributed clusters. This spectrum can be characterized along three axes: deployment
complexity, feature richness, and operational cost. Internet-scale systems such as Redis,
Memcached, Hazelcast, and Ignite occupy the high end of all three axes, providing sophisticated
capabilities at correspondingly high operational investment.

\subsection{The Underserved Middle}

Between lightweight in-process caches (Caffeine~\cite{caffeine2024}, Ehcache~\cite{ehcache2024})
and internet-scale systems lies a significant gap. Many real-world deployments serve tens to
hundreds of users, manage datasets measured in gigabytes rather than terabytes, and process
thousands---not millions---of operations per second. These workloads include: internal development
and testing environments, departmental dashboards and tools, early-stage startup infrastructure,
data pipeline staging caches, and compliance-sensitive applications requiring built-in security.

For such workloads, deploying Redis Cluster with Sentinel, or configuring Hazelcast with
discovery and partition management, represents significant over-engineering. The operational
investment in monitoring, backup, security configuration, and administration often exceeds
the value of the caching layer itself.

\subsection{Licensing Disruption}

The Redis licensing transition has created particular uncertainty. Redis Ltd.'s shift from
BSD to dual RSALv2/SSPLv1 licensing in March 2024~\cite{redis2024license} restricted cloud
service providers and triggered the creation of Valkey under the Linux Foundation, Redict
under LGPL, and KeyDB (previously acquired by Snap)~\cite{infq2024redis}. The subsequent
addition of AGPLv3 with Redis 8 in May 2025~\cite{redis2025license} attempted to address
community concerns but added further complexity to licensing evaluation. Organizations with
strict open-source policies now face a fragmented landscape with unclear long-term trajectories.

% ============================================================
% 3. Related Work
% ============================================================
\section{Related Work}
\label{sec:related}

We survey existing caching systems across four categories: foundational key-value stores,
distributed in-memory data grids, lightweight single-node caches, and emerging alternatives.

\subsection{Foundational Key-Value Stores}

\textbf{Memcached}~\cite{fitzpatrick2004} pioneered distributed caching with a simple,
multi-threaded architecture using consistent hashing for horizontal scaling. Its simplicity
remains compelling---a single binary with minimal configuration---but fundamental limitations
constrain its applicability: no persistence mechanism (data lost on restart), no built-in
replication, no authentication or access control, and support limited to simple key-value
strings. Nishtala et al.~\cite{nishtala2013} documented Facebook's extensive engineering
effort to scale Memcached, including custom replication layers and invalidation protocols
that far exceed the base system's capabilities.

\textbf{Redis}~\cite{carlson2013} extended the key-value paradigm with rich data structures
(strings, hashes, lists, sets, sorted sets, streams, bitmaps, HyperLogLog), persistence
via RDB snapshots and AOF logging, built-in replication, Lua scripting, pub/sub messaging,
and cluster mode with automatic sharding. Redis has become the de facto standard for
application caching, with AWS ElastiCache~\cite{elasticache2024} and similar managed services
providing hosted deployments. However, its licensing evolution---from BSD to RSALv2/SSPLv1
(March 2024)~\cite{redis2024license} to triple-licensed with AGPLv3 (Redis 8, May
2025)~\cite{redis2025license}---has introduced adoption risk. Community forks have emerged:
Valkey~\cite{valkey2024} (BSD, Linux Foundation backing) preserves the pre-license-change
codebase, while Redict (LGPL) and KeyDB~\cite{keydb2024} (BSD, multi-threaded) offer
alternative trajectories.

\subsection{Distributed In-Memory Data Grids}

\textbf{Hazelcast}~\cite{johns2015} provides a Java-based In-Memory Data Grid (IMDG) with
JCache (JSR-107) compliance, distributed data structures, CP subsystem based on Raft
consensus, and compute grid capabilities. Hazelcast excels in embedded Java deployments
and provides sophisticated cluster management with split-brain protection. However,
enterprise features including TLS/SSL, RBAC, persistence, and WAN replication require
commercial licensing, creating a significant cost barrier for smaller organizations.
A DZone comparison~\cite{dzone2024comparison} with Apache Ignite highlights the trade-offs
between Hazelcast's operational simplicity and Ignite's broader feature set.

\textbf{Apache Ignite}~\cite{ignite2024} provides a memory-centric distributed platform
combining in-memory caching, distributed SQL (ANSI-99 compliant), ACID transactions,
compute grid, machine learning capabilities, and native persistence. Ignite's breadth
is unmatched, but this comprehensiveness introduces significant operational complexity.
Dean and Barroso~\cite{dean2013} noted the challenges of tail latency in distributed
systems---challenges that Ignite's multi-layer architecture can amplify. The learning
curve for proper configuration of memory policies, persistence, and SQL indexing represents
a substantial investment.

\subsection{Lightweight Single-Node Caches}

\textbf{Ehcache}~\cite{ehcache2024} provides mature local caching with tiered storage
(heap, off-heap, disk), deep integration with Hibernate and Spring frameworks, and JCache
compliance. Ehcache 3 with Terracotta provides clustering capabilities, but the single-node
version remains most widely deployed. Ehcache serves primarily as an embedded library rather
than a standalone caching service, limiting its applicability for polyglot architectures.

\textbf{Caffeine}~\cite{caffeine2024} is a high-performance single-node Java cache using
the Window TinyLFU eviction policy, achieving near-optimal hit rates with O(1) operations.
Caffeine is excellent for in-process caching but provides no persistence, no network access
protocol, no administration interface, and no multi-language support. It serves as a building
block rather than a complete caching solution.

\subsection{Emerging Alternatives}

\textbf{Dragonfly}~\cite{dragonfly2024} is a modern Redis-compatible cache claiming 25x
throughput improvement through a shared-nothing multi-threaded architecture with io\_uring.
While impressive for high-throughput scenarios, Dragonfly targets the same internet-scale
segment as Redis.

\textbf{KeyDB}~\cite{keydb2024} (now part of Snap) provides a multi-threaded Redis fork
with active replication and FLASH storage support under BSD license. KeyDB addresses
Redis's single-threaded limitation but inherits its operational model.

\textbf{Valkey}~\cite{valkey2024} forked from Redis 7.2.4 following the license change,
maintaining BSD licensing under Linux Foundation governance. Valkey provides continuity
for the pre-change Redis codebase but represents a maintenance fork rather than an
architectural rethinking.

\textbf{Garnet}~\cite{garnet2024} from Microsoft Research is a high-performance cache-store
implemented in C\# using the .NET framework, designed for both cache and storage workloads.
Garnet targets cloud-native deployments with RESP protocol compatibility.

\subsection{Persistence Backend Technologies}

Kuber's pluggable persistence leverages established storage engines studied extensively
in the literature.

\textbf{RocksDB}~\cite{rocksdb2024} employs LSM-tree architecture originally described
by O'Neil et al.~\cite{oneil1996}, optimized for write-heavy workloads on flash storage.
RocksDB's compaction strategies, bloom filters, and configurable compression make it
suitable for datasets larger than available memory.

\textbf{LMDB}~\cite{chu2011} uses memory-mapped B+-trees with copy-on-write semantics,
providing zero-copy reads, ACID transactions without write-ahead logging, and
crash-resilient storage. Chu~\cite{chu2011} demonstrated LMDB's superior read performance
for datasets fitting in memory. Agrawal and Mamajiwala~\cite{agrawal2024} provided
comparative analysis of RocksDB, LMDB, and MongoDB for embedded storage, confirming
LMDB's read advantages and RocksDB's write throughput superiority.

\subsection{Consistency Models in Distributed Caches}

The CAP theorem, formally articulated by Brewer~\cite{brewer2000} and proved by
Gilbert and Lynch~\cite{gilbert2002}, establishes fundamental trade-offs in distributed
systems. Modern distributed caches navigate these trade-offs through various consistency
models. DeCandia et al.~\cite{decandia2007} (Amazon Dynamo) demonstrated eventual
consistency for high availability. Chang et al.~\cite{chang2006} (Google Bigtable) showed
strong consistency through distributed locking. Recent work by Zhang et al.~\cite{frontiers2025}
surveys consistency model evolution in distributed caching, noting the trend toward
configurable consistency per operation. Kuber's primary-secondary replication model
provides eventual consistency appropriate for its target workloads, avoiding the complexity
of distributed consensus protocols.

% ============================================================
% 4. System Architecture
% ============================================================
\section{System Architecture}
\label{sec:arch}

Kuber is architected as a single-JAR application with layered separation of concerns
between access protocols, cache service logic, persistence, and administration.

\subsection{High-Level Architecture}

The system comprises three converging access protocols feeding a unified cache service layer:

\begin{enumerate}[nosep]
  \item \textbf{Redis RESP Protocol:} TCP-based wire protocol providing drop-in compatibility
    with Redis clients across all major programming languages.
  \item \textbf{REST HTTP/JSON API:} HTTP endpoints supporting JSON-native operations
    including document storage, JSONPath queries, batch operations, and CSV export.
  \item \textbf{Message Broker Channels:} Asynchronous request-response messaging via
    Kafka, RabbitMQ, ActiveMQ, and IBM MQ with configurable SSL/TLS.
\end{enumerate}

All three protocols converge on a unified cache service layer that manages region-based
isolation, key indexing, value caching, persistence, and event publishing.

\subsection{Region-Based Isolation}

Each cache region operates as an independent unit with its own key index, value cache,
and persistence store. Regions are initialized concurrently at startup and provide
namespace isolation for multi-tenant or multi-application deployments. This design
enables per-region configuration of eviction policies, TTL defaults, persistence
backends, and security permissions.

\subsection{Hybrid Memory Model}
\label{sec:hybrid}

Kuber employs an Aerospike-inspired~\cite{aerospike2024} hybrid memory architecture
with the following characteristics:

\textbf{All keys always in memory:} The key index resides entirely in RAM, enabling
O(1) key existence checks (EXISTS), instant pattern matching (KEYS), and exact entry
counts (DBSIZE) without disk access. This eliminates the bloom-filter false positives
and disk-scanning overhead common in LSM-tree-based systems.

\textbf{Tiered value storage:} Hot values are cached in a Caffeine~\cite{caffeine2024}
LRU cache sized to available memory. Cold values reside exclusively in the persistence
backend, loaded on demand. This tiering enables datasets significantly larger than
available heap memory while maintaining sub-millisecond access for frequently used entries.

\textbf{Off-heap key index:} An optional DRAM-based key store outside the Java heap
supports key sets exceeding 2GB without garbage collection pressure. This addresses
a fundamental limitation of heap-based Java caches where large key sets trigger
stop-the-world GC pauses.

Table~\ref{tab:memory} summarizes the memory model.

\begin{table}[H]
\centering
\caption{Hybrid Memory Model Components}
\label{tab:memory}
\small
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Location} & \textbf{Purpose} \\
\midrule
Key Index & Heap / Off-heap & O(1) lookups, pattern match \\
Hot Values & Caffeine LRU & Sub-ms access, active data \\
Cold Values & Persistence backend & Capacity beyond memory \\
Off-Heap Store & Direct memory & GC-free key storage \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Pluggable Persistence}

Kuber supports five persistence backends, selectable per region:

\begin{table}[H]
\centering
\caption{Supported Persistence Backends}
\label{tab:persistence}
\small
\begin{tabular}{llp{3cm}}
\toprule
\textbf{Backend} & \textbf{Architecture} & \textbf{Best For} \\
\midrule
RocksDB & LSM-tree~\cite{oneil1996} & Write-heavy, large datasets \\
LMDB & B+-tree~\cite{chu2011} & Read-heavy, in-memory fits \\
SQLite & B-tree (SQL) & Simplicity, portability \\
MongoDB & Document store & JSON-native operations \\
PostgreSQL & MVCC relational & SQL integration needs \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Secondary Indexing}

Kuber implements two index types for JSON document queries: hash indexes providing O(1)
equality lookups, and B-tree indexes providing O(log n) range queries. Indexes are
maintained as hybrid structures combining in-memory operation with RocksDB-backed
persistence for durability.

% ============================================================
% 5. Multi-Protocol Access Layer
% ============================================================
\section{Multi-Protocol Access Layer}

\subsection{Redis Protocol (RESP)}

Kuber implements the Redis Serialization Protocol (RESP) for wire-level compatibility
with existing Redis clients. Standard Redis commands (GET, SET, DEL, EXISTS, EXPIRE,
KEYS, TTL, DBSIZE, PING, INFO, etc.) are supported, enabling drop-in replacement for
applications using Redis client libraries in Python, Java, C\#, Go, Ruby, Node.js, and
other languages.

\subsection{REST HTTP/JSON API}

The REST API provides JSON-native operations beyond traditional key-value semantics:

\begin{table}[H]
\centering
\caption{REST API Capabilities}
\label{tab:rest}
\small
\begin{tabular}{ll}
\toprule
\textbf{Operation} & \textbf{Description} \\
\midrule
Document Store & Store/retrieve JSON documents \\
JSONPath Query & Extract nested values via JSONPath \\
Criteria Search & Equality, IN, regex, range, not-equal \\
Batch Operations & Multi-key get/set/delete \\
JSON Merge & Partial update (JUPDATE) \\
CSV Export & Tabular export of cache contents \\
TTL Management & Per-entry time-to-live \\
Cross-Region Ops & Operations spanning regions \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Message Broker Access}

Kuber provides cache access through message brokers via asynchronous request-response
messaging. Supported brokers and their SSL/TLS configurations are:

\begin{table}[H]
\centering
\caption{Supported Message Brokers}
\label{tab:brokers}
\small
\begin{tabular}{ll}
\toprule
\textbf{Broker} & \textbf{SSL/TLS Modes} \\
\midrule
Apache Kafka & JKS, PEM, SASL\_SSL \\
RabbitMQ & JKS, PEM, mTLS \\
ActiveMQ & JKS, PEM \\
IBM MQ & JKS, mTLS\_JKS, mTLS\_PEM \\
\bottomrule
\end{tabular}
\end{table}

This enables architectures where cache access occurs entirely through existing messaging
infrastructure without additional network protocols or firewall rules---particularly
valuable in enterprise environments with strict network policies.

\subsection{Multi-Language Client Support}

Kuber provides client libraries and standalone demo applications for three languages:

\begin{itemize}[nosep]
  \item \textbf{Python:} Redis protocol (redis-py), REST API (requests/urllib), broker access
  \item \textbf{Java:} Redis protocol (Jedis/Lettuce), REST API (HttpClient), broker access
  \item \textbf{C\#/.NET:} Redis protocol (StackExchange.Redis), REST API (HttpClient), broker access
\end{itemize}

Each client supports all three access protocols. Demo applications use only standard
libraries to minimize dependency requirements.

% ============================================================
% 6. Capabilities
% ============================================================
\section{Capabilities}
\label{sec:capabilities}

\subsection{JSON Document Store}

Kuber provides first-class JSON document operations as core features rather than
optional modules:

\textbf{JSONPath Queries:} Extract nested values from stored JSON documents using
standard JSONPath expressions, enabling retrieval of specific fields without
transferring entire documents.

\textbf{Criteria-Based Search:} Query documents across a region using operators including
equality, IN (set membership), regex pattern matching, numeric range (greater-than,
less-than), and not-equal. Results support pagination and field projection.

\textbf{JSON Merge (JUPDATE):} Partial document updates via JSON merge patch semantics,
avoiding read-modify-write cycles for field-level modifications.

\textbf{Secondary Indexes:} Hash and B-tree indexes on JSON fields accelerate queries
from O(n) full scans to O(1) or O(log n) indexed lookups.

\subsection{Event Publishing}

Kuber publishes structured JSON events to message brokers on cache mutations:

\begin{itemize}[nosep]
  \item Event types: CREATE, UPDATE, DELETE, EXPIRE
  \item Per-region configuration with topic/queue targeting
  \item Fan-out to multiple broker/topic pairs per region
  \item SSL/TLS support: JKS, PEM, SASL\_SSL, mTLS\_JKS, mTLS\_PEM
  \item JSON-externalized configuration for runtime modification
\end{itemize}

This enables event-driven architectures where downstream systems react to cache changes
without polling.

\subsection{Security}

\textbf{Role-Based Access Control (RBAC):} Fine-grained permissions per cache region
with READ, WRITE, DELETE, and ADMIN roles. Users and roles are managed through the
web console or configuration files.

\textbf{API Key Authentication:} Machine-to-machine authentication via X-API-Key header,
Authorization: ApiKey bearer tokens, or query parameter. API keys are scoped to specific
regions and permission levels.

\textbf{Transport Security:} HTTPS for REST API and web console, TLS for Redis protocol
connections. SSL/TLS for all message broker connections with certificate management.

\textbf{Hot Reload:} Security configurations can be updated without server restart.

\subsection{Web Administration Console}

Kuber includes a Bootstrap-based web console providing:

\begin{itemize}[nosep]
  \item Cache statistics dashboard with real-time metrics
  \item Region management (create, configure, monitor)
  \item Entry browsing, editing, and JSON query interface
  \item Message broker management (live connect/disconnect)
  \item Event publishing and messaging channel configuration
  \item API key management with role-based scoping
  \item Integrated help documentation
  \item Dark/light theme support
  \item Prometheus metrics endpoint for monitoring integration
\end{itemize}

\subsection{Operational Features}

\textbf{Autoload:} Bulk import from CSV, TXT, and JSON files at startup or on demand.

\textbf{Backup/Restore:} Region-level backup and restore operations.

\textbf{Smart Memory Management:} Dual eviction combining heap watermark monitoring
with count-based entry limits. Warm object counts ensure minimum in-memory entries
for critical data.

\textbf{Compaction:} Pre-startup compaction for RocksDB and SQLite backends.
Scheduled compaction via cron expressions (default: 2 AM daily).

\textbf{Replication:} Primary-secondary replication via ZooKeeper coordination
with automatic failover.

% ============================================================
% 7. Feature Comparison
% ============================================================
\section{Feature Comparison}
\label{sec:comparison}

Table~\ref{tab:comparison} provides a qualitative comparison of Kuber against established
caching systems across 19 capability dimensions. The comparison focuses on architectural
capabilities rather than performance metrics, as Kuber targets a fundamentally different
workload profile than internet-scale systems.

\begin{table*}[t]
\centering
\caption{Feature Comparison Across Caching Systems (as of February 2026)}
\label{tab:comparison}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Capability} & \textbf{Kuber} & \textbf{Redis 8} & \textbf{Memcached} & \textbf{Hazelcast} & \textbf{Ignite} \\
\midrule
Single-JAR Deployment         & \checkmark & ---            & ---            & \checkmark     & --- \\
Redis Protocol (RESP)         & \checkmark & \checkmark     & ---            & ---            & --- \\
REST API (Native)             & \checkmark & ---            & ---            & \checkmark     & \checkmark \\
Message Broker Access         & \checkmark & ---            & ---            & ---            & --- \\
JSON Document Queries         & \checkmark & Module         & ---            & ---            & SQL \\
Secondary Indexes             & \checkmark & Module         & ---            & \checkmark     & \checkmark \\
Multi-Backend Persistence     & \checkmark & RDB/AOF        & ---            & Enterprise     & Native \\
Web Admin Console             & \checkmark & ---            & ---            & Mgmt Center    & --- \\
Built-in RBAC                 & \checkmark & ACL            & ---            & Enterprise     & --- \\
Event Publishing              & \checkmark & Pub/Sub        & ---            & Listeners      & Events \\
Broker-Based Messaging        & \checkmark & ---            & ---            & ---            & --- \\
Multi-Language Clients        & \checkmark & \checkmark     & \checkmark     & \checkmark     & \checkmark \\
Replication                   & \checkmark & \checkmark     & ---            & \checkmark     & \checkmark \\
SSL/TLS                       & \checkmark & \checkmark     & ---            & Enterprise     & \checkmark \\
Off-Heap Storage              & \checkmark & ---            & ---            & \checkmark     & \checkmark \\
Bulk Import (Autoload)        & \checkmark & ---            & ---            & ---            & --- \\
License (2025)                & Open       & AGPLv3/RSAL    & BSD            & Apache/Comm.   & Apache \\
Min. Deploy Nodes             & 1          & 1 (6 cluster)  & 1              & 1 (3 cluster)  & 1 (3 cluster) \\
External Dependencies         & JVM only   & None           & libevent       & JVM            & JVM \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Differentiators}

\textbf{Three-Protocol Convergence:} Kuber is unique among surveyed systems in providing
Redis RESP, REST HTTP/JSON, and message broker access through a single deployment. This
enables polyglot architectures where different services access the same cache through
their preferred protocol.

\textbf{JSON-Native Operations:} While Redis requires the RedisJSON module (not included
in the open-source base), and Memcached has no JSON support, Kuber provides JSON document
storage, JSONPath queries, criteria search, and secondary indexes as built-in features.

\textbf{Integrated Administration:} Redis relies on external tools (RedisInsight, redis-cli).
Memcached provides no web interface. Kuber includes a comprehensive web console with
region management, entry browsing, query interface, broker management, and API key
administration---all without additional software installation.

\textbf{Pluggable Persistence:} Five backend options allow matching storage characteristics
to workload requirements without changing application code. No other surveyed system
offers comparable backend flexibility in its open-source edition.

\textbf{Single-JAR Simplicity:} Kuber deploys as a single JAR file with no external
dependencies beyond a JVM runtime. No Docker containers, no cluster configuration,
no cloud provisioning required for immediate productivity.

% ============================================================
% 8. Target Use Cases
% ============================================================
\section{Target Use Cases}

Kuber is designed for workloads where operational simplicity outweighs raw throughput:

\textbf{Development and Testing:} Single JAR with integrated web console eliminates
the need for Docker Compose configurations, cluster provisioning, or external
administration tools during development.

\textbf{Small Teams and Departmental Applications:} Internal tools, dashboards,
and data pipelines serving tens to hundreds of users. The integrated RBAC, API keys,
and SSL/TLS provide production-grade security without enterprise licensing costs.

\textbf{Microservice Prototypes:} Multi-protocol flexibility enables architectural
evolution---starting with REST API for simplicity, migrating to Redis protocol for
performance, and adding broker-based access as the system matures.

\textbf{Message-Driven Architectures:} Organizations with existing Kafka, RabbitMQ,
or ActiveMQ infrastructure can access the cache through message brokers without
introducing additional network protocols or firewall rules.

\textbf{Compliance-Sensitive Environments:} Built-in RBAC, API key management,
SSL/TLS, and audit logging (via event publishing) address compliance requirements
that otherwise mandate enterprise-tier licensing from competing systems.

% ============================================================
% 9. Discussion
% ============================================================
\section{Discussion}

\subsection{Design Trade-offs}

\textbf{Single-Node Focus vs.\ Horizontal Scale:} Kuber prioritizes single-node
deployment simplicity over internet-scale horizontal scaling. Primary-secondary
replication provides redundancy and read scaling, but Kuber does not implement
distributed sharding or consensus protocols. This is a deliberate design choice
for the target workload segment.

\textbf{JVM Dependency vs.\ Universality:} Requiring a JVM runtime excludes
deployment scenarios where JVM overhead is unacceptable. However, JVM ubiquity
in enterprise environments minimizes this limitation for Kuber's target audience.

\textbf{Feature Breadth vs.\ Complexity:} Supporting three access protocols,
five persistence backends, and integrated administration increases codebase complexity.
This trade-off is justified by the operational simplicity it provides to users---a
single deployment replaces multiple specialized tools.

\subsection{Limitations}

\begin{enumerate}[nosep]
  \item \textbf{Throughput ceiling:} Kuber is not designed for workloads exceeding
    tens of thousands of operations per second. Internet-scale workloads should
    use Redis, Dragonfly, or similar purpose-built systems.
  \item \textbf{No distributed sharding:} Dataset size is bounded by single-node
    capacity. Replication provides redundancy but not horizontal data partitioning.
  \item \textbf{JVM memory overhead:} The JVM runtime introduces baseline memory
    consumption that may be significant for extremely resource-constrained environments.
  \item \textbf{Redis protocol subset:} Not all Redis commands are implemented.
    Advanced features (Lua scripting, streams, cluster commands) are not supported.
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}[nosep]
  \item \textbf{Distributed Mode:} Multi-node deployment with automatic region
    partitioning and consensus-based coordination.
  \item \textbf{WebAssembly Calculators:} Sandboxed server-side computation via
    WASM for user-defined transformations.
  \item \textbf{GraphQL API:} Query interface for complex JSON document relationships.
  \item \textbf{Cloud-Native Packaging:} Helm charts, Kubernetes operators, and
    managed service options.
\end{enumerate}

% ============================================================
% 10. Conclusion
% ============================================================
\section{Conclusion}

We presented Kuber, a lightweight distributed cache addressing the underserved market
segment of low-to-moderate volume caching workloads. Through a hybrid memory architecture
combining in-memory keys with tiered value storage, multi-protocol access via Redis RESP,
REST HTTP/JSON, and message broker channels, and an integrated web administration console
with built-in security, Kuber provides capabilities previously requiring either enterprise
licensing or complex multi-system deployments---all in a single-JAR package.

Kuber does not compete with internet-scale systems on throughput or horizontal scalability.
Instead, it offers a practical, low-cost alternative for development teams, departmental
applications, early-stage startups, and compliance-sensitive environments where operational
simplicity and feature completeness outweigh raw performance. The open-source license,
zero external dependencies beyond JVM, and integrated administration make Kuber
immediately deployable without infrastructure investment.

Kuber is open-source and available at \url{https://github.com/ajsinha/kuber}.

% ============================================================
% Acknowledgments
% ============================================================
\section*{Acknowledgments}

The author thanks the early adopters and contributors to the Kuber project for their
valuable feedback, testing, and feature suggestions that have shaped the system's
architecture and capabilities.

% ============================================================
% References
% ============================================================
\begin{thebibliography}{99}

\bibitem{fitzpatrick2004}
B.~Fitzpatrick, ``Distributed caching with Memcached,'' \emph{Linux Journal}, vol.~2004,
no.~124, 2004.

\bibitem{carlson2013}
J.~L.~Carlson, \emph{Redis in Action}. Manning Publications, 2013.

\bibitem{nishtala2013}
R.~Nishtala et~al., ``Scaling Memcache at Facebook,'' in \emph{Proc. USENIX NSDI}, 2013.

\bibitem{johns2015}
M.~Johns, \emph{Getting Started with Hazelcast}, 2nd ed. Packt Publishing, 2015.

\bibitem{ignite2024}
Apache Software Foundation, ``Apache Ignite Documentation,''
\url{https://ignite.apache.org/docs/latest/}, 2024.

\bibitem{redis2024license}
R.~Peralta, ``Redis Adopts Dual Source-Available Licensing,'' Redis Blog,
\url{https://redis.io/blog/redis-adopts-dual-source-available-licensing/}, Mar.~2024.

\bibitem{redis2025license}
Redis Ltd., ``Redis 8 Licensing Update: Adding AGPLv3,'' Redis Blog,
\url{https://redis.io/blog/agplv3/}, May~2025.

\bibitem{valkey2024}
Linux Foundation, ``Valkey: An Open Source Alternative to Redis,''
\url{https://valkey.io/}, 2024.

\bibitem{infq2024redis}
R.~Katz, ``The Redis Licensing Saga and Its Forks,'' InfoQ,
\url{https://www.infoq.com/news/2024/04/redis-license-change/}, 2024.

\bibitem{keydb2024}
Snap Inc., ``KeyDB: A Multithreaded Fork of Redis,''
\url{https://docs.keydb.dev/}, 2024.

\bibitem{caffeine2024}
B.~Manes, ``Caffeine: A High Performance Caching Library for Java,''
\url{https://github.com/ben-manes/caffeine}, 2024.

\bibitem{ehcache2024}
Software AG, ``Ehcache,'' \url{https://www.ehcache.org/}, 2024.

\bibitem{dragonfly2024}
DragonflyDB, ``Dragonfly: A Modern Redis Replacement,''
\url{https://www.dragonflydb.io/}, 2024.

\bibitem{garnet2024}
Microsoft Research, ``Garnet: A High-Performance Cache-Store,''
\url{https://github.com/microsoft/garnet}, 2024.

\bibitem{rocksdb2024}
Meta, ``RocksDB: A Persistent Key-Value Store,''
\url{https://rocksdb.org/}, 2024.

\bibitem{chu2011}
H.~Chu, ``LMDB: Lightning Memory-Mapped Database,'' in \emph{LDAPCon}, 2011.
\url{https://www.symas.com/lmdb}

\bibitem{agrawal2024}
N.~Agrawal and S.~Mamajiwala, ``Comparative Analysis of RocksDB, LMDB, and MongoDB
for Embedded Storage,'' \emph{SAE International Journal}, 2024.

\bibitem{oneil1996}
P.~O'Neil et~al., ``The Log-Structured Merge-Tree (LSM-Tree),'' \emph{Acta Informatica},
vol.~33, no.~4, pp.~351--385, 1996.

\bibitem{brewer2000}
E.~Brewer, ``Towards Robust Distributed Systems,'' in \emph{Proc. ACM PODC}, 2000.

\bibitem{gilbert2002}
S.~Gilbert and N.~Lynch, ``Brewer's Conjecture and the Feasibility of Consistent,
Available, Partition-Tolerant Web Services,'' \emph{ACM SIGACT News}, vol.~33, no.~2,
pp.~51--59, 2002.

\bibitem{decandia2007}
G.~DeCandia et~al., ``Dynamo: Amazon's Highly Available Key-Value Store,'' in
\emph{Proc. ACM SOSP}, 2007.

\bibitem{chang2006}
F.~Chang et~al., ``Bigtable: A Distributed Storage System for Structured Data,'' in
\emph{Proc. USENIX OSDI}, 2006.

\bibitem{dean2013}
J.~Dean and L.~A.~Barroso, ``The Tail at Scale,'' \emph{Communications of the ACM},
vol.~56, no.~2, pp.~74--80, 2013.

\bibitem{aerospike2024}
Aerospike, ``Hybrid Memory Architecture,''
\url{https://aerospike.com/docs/architecture/hybrid-memory}, 2024.

\bibitem{elasticache2024}
Amazon Web Services, ``Amazon ElastiCache,''
\url{https://aws.amazon.com/elasticache/}, 2024.

\bibitem{dzone2024comparison}
DZone, ``Apache Ignite vs Hazelcast: Feature Comparison,''
\url{https://dzone.com/articles/apache-ignite-vs-hazelcast}, 2024.

\bibitem{frontiers2025}
Y.~Zhang et~al., ``Consistency Models in Distributed Caching: A Survey,''
\emph{Frontiers in Computing}, 2025.

\end{thebibliography}

% ============================================================
% Disclaimer and Legal Notice
% ============================================================
\onecolumn
\section*{Disclaimer and Legal Notice}

\subsection*{DISCLAIMER OF WARRANTIES}

\begin{small}
THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR
PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE
FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
\end{small}

\subsection*{LIMITATION OF LIABILITY}

\begin{small}
TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT SHALL THE AUTHOR,
CONTRIBUTORS, OR COPYRIGHT HOLDERS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.
\end{small}

\subsection*{USAGE RESPONSIBILITY}

Users of Kuber are solely responsible for:
\begin{itemize}[nosep]
  \item Ensuring compliance with all applicable laws and regulations in their jurisdiction
  \item Validating the software's suitability for their specific use case
  \item Implementing appropriate security measures for production deployments
  \item Backing up data and maintaining disaster recovery procedures
  \item Testing thoroughly in non-production environments before deployment
\end{itemize}

\subsection*{NO PROFESSIONAL ADVICE}

This software and documentation do not constitute professional advice of any kind.
Users should consult with qualified professionals for legal, financial, security,
or other specialized guidance.

\subsection*{PATENT NOTICE}

\begin{small}
Patent Pending: Certain architectural patterns and implementations described in this paper
may be subject to patent applications, including but not limited to: the hybrid memory
architecture with in-memory key index and tiered value storage, the multi-protocol access
layer combining Redis RESP, REST HTTP/JSON, and message broker channels, the region-isolated
persistence model with pluggable backends, and the integrated broker-based cache access
with structured event publishing.
\end{small}

\subsection*{TRADEMARK NOTICE}

\begin{small}
Redis is a registered trademark of Redis Ltd. Apache Kafka, Apache Ignite, Apache ActiveMQ,
and Apache Flink are trademarks of the Apache Software Foundation. RabbitMQ is a trademark
of Broadcom Inc. IBM MQ is a trademark of International Business Machines Corporation.
Memcached is a trademark of Danga Interactive. Hazelcast is a trademark of Hazelcast, Inc.
Aerospike is a trademark of Aerospike, Inc. All other trademarks are the property of their
respective owners.
\end{small}

\subsection*{THIRD-PARTY COMPONENTS}

Kuber incorporates third-party open-source components, each subject to their respective
licenses. Users are responsible for compliance with all applicable third-party licenses.

\subsection*{INDEMNIFICATION}

By using this software, you agree to indemnify and hold harmless the author and contributors
from any claims, damages, or expenses arising from your use of the software.

\subsection*{ACCURACY DISCLAIMER}

\begin{small}
Feature comparisons in this paper are based on publicly available documentation as of
February 2026. The author makes no warranties regarding the accuracy of third-party product
descriptions. Readers should consult official documentation for definitive feature information.
This paper is provided for informational purposes only.
\end{small}

\vspace{1em}
\begin{center}
\textcopyright\ 2025--2030 Ashutosh Sinha. All rights reserved.\\
Contact: \texttt{ajsinha@gmail.com}\\
Repository: \url{https://github.com/ajsinha/kuber}
\end{center}

\end{document}
